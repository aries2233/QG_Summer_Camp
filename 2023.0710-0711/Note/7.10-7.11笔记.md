# 7.10-7.11笔记

## BP神经网络

bp是 Back Propagation 的简写 ，意思是反向传播。



输入层（input） ， 隐藏层 （hidden），输出层（output）和权重（weight） 。而所有的网络都可以理解为由这三层和各层之间的权重组成的网络，只是隐藏层的层数和节点数会多很多。

人们后来对节点上的数据进行了一个操作，利用sigmoid()函数来处理，使数据被限定在一定范围内。此外sigmoid函数的图像是一个非线性的曲线，因此，能够更好的逼近非线性的关系，因为绝大多数情况下，实际的关系是非线性的。sigmoid在这里被称为 激励函数 ，这是神经网络中的一个非常重要的基本概念。

符号 h1bhb1，h2bhb2，ybyb 。它们也各自代表一个矩阵，它们的概念为阈值，通常用符号b来表示。阈值的意义是，每个节点本身就具有的一个数值，设置阈值能够使网络更快更真实的去逼近一个真实的关系。

真实结果与计算结果的误差被称作 损失 loss ， loss = | - y|  记作 损失函数 。这里有提到了一个很重要的概念，损失函数，其实在刚才的例子中，损失函数 loss = | - y|  只是衡量误差大小的一种方式，称作L1损失（先知道就行了），在实际搭建的网络中，更多的用到的损失函数为 均方差损失，和交叉熵损失。原则是分类问题用交叉熵，回归问题用均方差，综合问题用综合损失，特殊问题用特殊损失···

## git多人协作

**多人协作一定要用 .gitignore 把不需要跟踪的文件筛掉。**



# CNN

卷积神经网络（CNN），这是深度学习算法应用最成功的领域之一，卷积神经网络包括一维卷积神经网络，二维卷积神经网络以及三维卷积神经网络。一维卷积神经网络主要用于序列类的数据处理，二维卷积神经网络常应用于图像类文本的识别，三维卷积神经网络主要应用于医学图像以及视频类数据识别。



## 卷积->池化->全连接



实战案例：https://blog.csdn.net/gary101818/article/details/122458932